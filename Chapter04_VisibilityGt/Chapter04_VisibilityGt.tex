\section{Visibility graphs {\bf{(Jonathan)}}}\label{sec:VisibilityGt}
	\subsection{Historical roots}
	
	Among other approaches, the so-called visibility graph (VG) has attracted considerable interest. Originally, this concept has been introduced for the analysis of mutual visibility relationships between points and obstacles in two-dimensional landscapes in the framework of computational geometry, with applications ranging from robot motion planning to architectural design and topographic descriptions of geo-graphical space \cite{Lozano1979,Nagy1994,Floriani1994,Turner2001}.  Lacasa \emph{et al.}\cite{Lacasa2008} adopted the VG approach to the analysis of structures in scalar, univariate time series. It has been shown that certain statistical features of the resulting complex networks are closely related with fractal and multi-fractal properties of the underlying time series \cite{Lacasa2009,Ni2009}. A simplifying methodological variant called horizontal visibility graphs has been proposed (HVGs, see \cite{Luque2009,Lacasa2010,Xie2011,Gutin2011}). 

	A mini review of (H)VGs has been presented in \cite{Nunez2012,Luque2016}, in particular, the application of this approach to geophysical processes has been reported in \cite{Donner2012}, which link the complete variety of different network properties describing the structure of VGs with specific structural features of geophysical processes in some more detail. Here we summarize the recent developments of the method and discuss some practical issues which pose considerable challenges to VG analysis of geophysical data, such as missing data, homo- and heteroscedastic uncertainty of observations, and time-scale uncertainty. We also discuss some successful applications to testing time irreversibility of nonlinear time series. 
	
	\subsection{Algorithmic variants}
	In (H)VG analysis, individual observations are considered as vertices. For instance, given a univariate time series $x(t_i)_{i= 1, \ldots,N}$, the binary adjacency matrix $A$ has dimension ${N \times N}$. Depending on the particular visibility conditions in defining the edges of the resulted graph, we have several different versions of VGs. 
		\subsubsection{Natural visibility graphs}
		First, in the framework of the standard visibility graph (VG), the non-zero entries of $A_{i,j}$ correspond to two time points $t_i$ and $t_j$ are mutually connected vertices if the following criterion 
\begin{equation} \label{vis_cond}
	\frac{x(t_i)-x(t_k)}{t_k - t_i} > \frac{x(t_i) - x(t_j)}{t_j - t_i}
\end{equation}
is fulfilled for all time points $t_k$ with $t_i < t_k < t_j$~\cite{Lacasa2008}. Therefore, the edges of the network take into account the temporal information explicitly.  In Fig. \ref{fig_chap04:timeseriesSS}(a), we show the algorithm of constructing VGs for almost periodic sunspot series. More detailed discussions on VGs analysis for sunspot series will be reviewed in Sec. \ref{sec:appVGs}. By default, two consecutive observations are connected and the graph forms a completely connected component without disjoint subgraphs. There are pronounced boundary effects, for instance, the first time point can only be visible to points that are in the future time axis (Fig. \ref{fig_chap04:timeseriesSS}(a)). Furthermore, the VG is not affected by choice of algorithmic parameters -- in contrast to most other methods of constructing complex networks from time series data are dependent on the choice of some parameters ({\emph{e.g.}} the threshold $\varepsilon$ of recurrence networks, see more details in~\cite{Donner2010a}). 
		\begin{figure}
		  \centering
		  \includegraphics[width=0.8\columnwidth]{Chapter04_VisibilityGt/TSspotnumberYear.eps}
		  \caption{Algorithm of constructing natural visibility graphs (a), and (b) horizontal VG for time series of sunspot number series. Reproduced from \cite{Zou2014}. \label{fig_chap04:timeseriesSS}}
		\end{figure}
		
		\subsubsection{Horizontal visibility graphs}
		As a notable modification of the standard VG algorithm, Luque \textit{et al.} \cite{Luque2009} proposed utilizing a simplified criterion of horizontal visibility for transforming a time series into a complex network. Specifically, they considered two observations made at times $t_i$ and $t_j$ to be connected in a horizontal visibility graph (HVG) if and only if
\begin{align}
x(t_k) < \min \left( x \left(t_i \right), x \left(t_j \right) \right) \label{eq:hvc}
\end{align}
for all $t_k$ with $t_i<t_k<t_j$. 

		The algorithmic difference between HVG and VG is illustrated in Fig. \ref{fig_chap04:timeseriesSS}(b). Note that the geometric criterion defined for the HVG algorithm is more ``visibility restrictive" than its analogous for the standard VG. That is to say, the nodes within the HVG will have ``less visibility" than their counterparts within the VG. It is easily seen that the edge set of the HVG associated with a given time series is a subset of the edge set of the associated VG (i.e., if the horizontal visibility criterion Eq.~(\ref{eq:hvc}) is fulfilled, then also Eq. (\ref{vis_cond}) holds, but not necessarily vice versa). In addition, VGs are invariant under affine transformations of the entire time series, whereas HVGs are not. One notable advantage of HVGs is that they provide an even higher degree of algorithmic simplicity than standard VGs, resulting in the observation that for certain simple stochastic processes and the quasiperiodic transition route to chaos, some basic graph properties can be calculated analytically \citep{Luque2009,Luque2013a,Luque2013d}. On the other hand, the fact that HVGs typically contain a lower number of edges increases the demands regarding the time series length relatively to the standard VG when using this approach in applications such as tests for time-reversal asymmetry \citep{Donges2013}. 
		
		\subsubsection{Other variants of (H)VG}
		Given the definitions of VG and HVG, the resulting graphs are undirected, unweighted. One straightforward generalization of (H)VGs to directional (H)VGs is to introduce directed edges between vertices, i.e., from the cause at $t_i$ to the effect at $t_j > t_i$. As it will be shown in Sec. \ref{sec:timeIRvg}, such directed graphs will give information on time-reversal asymmetry of the considered time series since the connection from $t_i$ to $t_j$ does not necessarily coincident with the connection from $t_j$ to $t_i$. 
		
		There are some further variants of (H)VG depending on the particular properties of the given time series. For instance, given a binary series, in \cite{Ahadpour2012}, a simplified VG has been obtained for binary VG and the visibility condition (expression \eqref{vis_cond}) is reduced to $x(t_i) + x(t_j) > x(t_k)$ that $x(t_k) = 0$ for all $t_k$ such that $t_i < t_k < t_j$. The resulting VG from binary series is always connected and undirected, which is more solvable than the standard VGs. 
		
		Parametric VG has been proposed in \cite{Bezsudnov2014,Snarskii2013a}, introducing "view angle" $\alpha$. When the angle $\alpha = \pi$, the parametric version VG and the standard VG are the same. However, the angle $\alpha = \pi / 2$ does not turn into HVG because $\alpha$ introduces a direction of links in the resulted graphs. The next step is to study the dependence of network structural measures on the parameter $\alpha$. 
		
		Multiscale limited penetrable HVG algorithm has been proposed in \cite{Gao2016,Pei2014a}, which can be regarded as a continuous construction of HVG based on a proper coarse grained time series. In the original HVG, two time points $t_i$ and $t_j$ are connected if no other intermediate points $t_k$ are larger than $\min{(x(t_i), x(t_j))}$, namely, $t_k < \min{x(t_i), x(t_j)}, i < k < j$. Now we use a less restrictive criterion, allowing one of $t_k$ being larger than $\min{(x(t_i), x(t_j))}$, as represented by the new parameter of $L=1$. We allow two points of $t_k$ larger than $\min{(x(t_i), x(t_j))}$ if $L = 2$. When increasing $L$, there are more edges in the resulting HVG comparing to the standard HVG. The standard VG is recovered from HVG if $L \to \infty$. Furthermore, the limited penetrable VG algorithm has been combined with parametric VG in \cite{Li2018}. 
				
		The extension of (H)VGs from a univariate time series to scalar fields has been recently reported in \cite{Xiao2014a,Lacasa2017}, which is conceptually closer to the original idea of visibility graphs. In addition, one may reconstruct (H)VGs for a set of ordered data (either descending or increasing ordered) \cite{Wang2015a}. 
						
		It is also possible to combine the concepts of VGs with transition networks (Sec. \ref{sec:TransitionNt}), for instance, the so-called visibility graphlet approach has been proposed in \cite{Mutua2015,Mutua2016a}. In this case, VGs are constructed respectively for sliding windows and each window is regarded as a network node (phase space partition). The connection between two nodes are decided by the temporal succession. The resulting transition network shows distinct topological features for chaotic series \cite{Mutua2016a}. 
		
		In all above cases, (H)VGs are reconstructed from a given time series. In \cite{Tsiotas2018}, {\textit {Tsiotas et al.}} expanded the VG algorithm to analyze node's attributes of a given graph. More specifically, let us consider a graph $G(V, E)$ and a node-wise attribute $Y$ that might be any of the node-wise network measures, for instance, local clustering coefficient or betweenness centralities. The secondary VG analysis for node-wise attributes $Y$ shows specific ability in pattern recognitions \cite{Tsiotas2018}. 
				
		Other properties of combinatorics have been used to characterize HVGs successfully in \cite{Gutin2011}. Recently, some analytic results have been obtained for some independent and identically distributed random noise, which has exponential degree distribution \cite{Wang2018}. 

		The time complexity of the basic natural VG algorithm is $O(N^2)$, which means that it takes a lot of time when dealing with long time series. A faster transform algorithm has been proposed in \cite{Lan2015a} to reduce the computation time, showing much efficient time complexity $O(N \log N)$. Note that for the HVG algorithm, it is not possible to further improve the computation efficiency because the computation complexity has reached the lower bound $O(N)$. 

	\subsection{Visibility graph properties}
		\subsubsection{Degree distributions}
		Recent work on (H)VGs has mainly concentrated on the properties of the degree distribution $p(k)$ resulting from different kind of processes. Specifically, VGs obtained from periodic signals appear as a concatenation of a finite number of network motifs (given that the basic period is an integer multiple of the sampling rate), i.e., have a regular structure with only a few distinct values of the vertex degree. The opposite extreme case, white noise, yields VGs appearing as exponential random graphs, i.e., random networks characterized by an exponential degree distribution. For example, exponential degree distribution has been tested in wind speed records measured in central Argentina \cite{Pierini2012}. 
		
		In fractal processes, numerical results suggest that $p(k)$ exhibits a power law \cite{Lacasa2008}, $p(k) \sim k^{-\gamma}$. Taking this empirical observation, VG analysis has been suggested to characterize fractional Brownian motions and $f^{\beta}$-noise, finding some heuristic relationship between $\gamma$ and the process Hurst exponent $H$ as $\gamma = 3 - 2H$, and $\gamma = 5 - 2H$ for fractional Gaussian noise \cite{Lacasa2009,Ni2009}. Depending on the fractal properties of the underlying process, recently an algorithm for constructing VGs from segmented time series has been proposed \cite{Ahmadlou2012}, which estimates power-law exponents reflecting scale-free properties quite well. The idea hinges on a proper choice of the time delay $\tau$, which re-samples the original time series resulting in a number of segmentations. However, the number of segments to be used is often not known in the original algorithm. In other words, we do not know the upper bound of $\tau_{max}$ to terminate the computation. So far, only a heuristic choice of $\tau_{max}$ has been suggested when network characteristics of segmented time series show more or less convergent behavior \cite{Ahmadlou2012}. This improved algorithm has been applied to diagnose Autism spectrum disorders \cite{Ahmadlou2012}. Since there are much concerns regarding the statistical justification of the power laws of VGs, one can directly analyze the degree sequence (instead of the distribution) by detrended fluctuation analysis \cite{Czechowski2016}, which quantifies multifractal properties better than the standard VGs analysis.  
	
		Some exact results of $p(k)$ of the HVG associated with generic uncorrelated random series have been obtained in \cite{Luque2009}. More specifically, for a bi-infinite time series created from a random variable $X$ with probability distribution $f(x)$ with $x\in [0, 1]$, it has been proved that the degree distribution of the graph has an exponential form 
		\begin{equation}\label{pkhvg}
		p(k) = \frac{1}{3} (\frac{2}{3})^{k-2}, 
		\end{equation}
where $k$ is the degree. Interestingly, every probability distribution $f(x)$ of uncorrelated random series have the same exponential form. Numerical results of $f(x)$ from a uniform, a Gaussian and a power law form (e.g., $f(x) \sim x^{-2}$) show perfect agreements with this theoretical predictions \cite{Luque2009}. A general diagrammatic theory has been proposed in \cite{Lacasa2014b} to compute $p(k)$ for any given dynamical process with well defined invariant measure. Taking into account the time information explicitly as in so-called directed HVG (as will be explained below), the outgoing degree distribution $p_{out}(k) = (1/2)^{k}$. Further solvable example are Markovian processes with an integrable invariant measure $f(x)$, for instance, the stationary Ornstein-Uhlenneck process, one dimensional chaotic and quasiperiodic maps of with smooth invariant measure. In addition, the mean degree $\bar{k}$ of the HVG associated to an uncorrelated random process is then given by 
\begin{equation}
\bar{k} = \sum k p(k) = \sum_{k=2}{\infty} \frac{k}{3}{(\frac{2}{3})}^{k-2} = 4.
\end{equation}
For an infinite periodic series of period $T$, the mean degree $\bar{k}$ is $\bar{k} = 4 ( 1 - \frac{1}{2T})$. 
	
		Nevertheless, similar to VGs, HVGs have been successfully applied to studying time series from various fields of sciences. Within the area of the present manuscript, we particularly notice the recent paper by \cite{Yu2012} who studied the multifractal properties of some solar flare index in terms of HVG characteristics. The properties of HVG have been tested in river flows \cite{Braga2016}, showing exponential degree distributions. 
		
		In order to understand the hypothetical scale-free property of $p(k)$ for VGs from fractal records, one has to note that typically, maxima of the time series have visibility contact with more other vertices than other points, i.e., hubs of the network often form at maximum values of the recorded observable. Put it differently, the degree of a vertex in the VG characterizes the maximality property of the corresponding observation in comparison with its neighborhood in the time series. Although local large time series values have better visibility than other small values, hub nodes of large degrees of VGs do not necessarily correspond to higher values, especially when there are some sort of periodic trends in the given data sequence, for instance wind speed records \cite{Pierini2012,Zou2014a}. Therefore, the relationship between maxima time series points and hubs of VGs is not completely general, since there can be specific conditions (e.g., a concave behavior over a certain period of time) which can lead to highly connected vertices that do not coincide with local maxima, for example, in case of a Conway series \cite{Lacasa2008}. 
				
		In addition, minima of time series provide complementary insight for the understanding of the particular process, for instance, sun spot series \cite{Zou2014a}. In the standard VGs, the contributions of local minimum values have been somehow largely overlooked by degree distribution $p(k)$ because minimum values are basically non-hubs. One simple solution is to study the negatively inverted counterpart of the original time series, namely, $-x(t_i)$, which quantifies the properties of the local minima \cite{Zou2014a}. For convenience, we use $k_{-x}$ and $p(k_{-x})$ to denote degree sequence and distribution of VG resulted from $-x(t_i)$. Here, we remark that this simple inversion of the time series allows us to create an entirely different complex network. This technique has demonstrated to be useful to understand the long-term behavior of strong minima of the solar cycles \cite{Zou2014a}. We will review these results in Sec. \ref{subsec:sunnum}. 
		
		Based on degree distribution $p(k)$, we can calculate the graph entropy $h = - \sum_{k} p(k) \log p(k)$, which is used as the approximation to the Shannon entropy $H$ of the corresponding time series $x(t)$ \cite{Luque2016,Goncalves2016}. Furthermore, the VG aggregation operator has been proposed in \cite{Chen2014,Jiang2016}. This operator includes the temporal information in the weights of the aggregation, showing computational simplicity comparing to other traditional aggregation operators.  
					
		\subsubsection{Stochastic vs. deterministic dynamics}
		Concerning the HVG, exponential functional forms have been obtained for many random processes, namely, $p(k) \sim e ^{-\lambda k}$. A scaling factor of $\lambda_c = \ln (3/2)$ has been found in the case of uncorrelated noise (white noise), which has been further proposed to separate stochastic from chaotic dynamics in the following senses \cite{Lacasa2010,Lacasa2014b,Ravetti2014}: (i) correlated stochastic series are characterized by $\lambda > \lambda_c$, slowly tending to an asymptotic value of $\ln (3/2)$ for very weak correlations, whereas (ii) chaotic series are often characterized by $\lambda_{chaos} < \lambda_c$ for decreasing correlations or increasing chaos dimensionality, respectively \cite{Lacasa2010}. In \cite{Zhang2017}, we have provided some further examples supporting argument (i). Meanwhile, we showed some peculiar results indicating that $\lambda_c$ should not be interpreted as a general critical value separating chaos from noise. 
		
		Let us focus on applying (H)VG analysis to auto-regressive (AR) stochastic processes, which often describe certain time-varying processes in nature, economics, etc. The AR model specifies that the output variable depends linearly on its own previous values and on a stochastic term. More specifically, ${\bf x} = [x_1, x_2, \ldots, x_i, \ldots, i \in \mathbb{Z} ]$ is an AR model of order $p$ denoted as AR($p$) if
\begin{equation} \label{def:AR}
x_t = \sum_{j=1}^{p}\varphi_j x_{t-j} + \varepsilon_t, 
\end{equation}
where $\varphi_j, j \in [1, p],$ are real-valued coefficients of the model, and $\varepsilon_t$ is white noise. We further assume that the error terms $\varepsilon_t$ follow a Gaussian distribution with zero mean and unit variance. Specifically, we perform both VG and HVG analysis for AR(1), namely, (i) $|\varphi_1| < 1$ for the AR(1) model. It is known that $\varphi_1 > 0$ corresponds to positive correlation and the correlation length increases when $\varphi_1$ is increased from 0 to 1. In contrast, anti-correlation is observed for negative coefficient $\varphi_1$. Similar H(VG) analysis for the AR(2) model have reported in \cite{Zhang2017}. 

		In the case of $\varphi_1 > 0$, we find that $p(k)$ approximately follows an exponential distribution. To illustrate this finding, the cumulative degree distributions $F(k)$ for $\varphi_1 = 0.3 $, $0.9$ and $-0.5$ are shown in Fig. \ref{fig:lambda_AR1}A and B, where clear scaling regimes are present in the semi-log plots. Furthermore, when increasing $\varphi_1$, in the VG, the exponent $\lambda$ shows a monotonically decreasing trend (Fig. \ref{fig:lambda_AR1}C). In contrast, the value $\lambda$ for the HVG is increased (Fig. \ref{fig:lambda_AR1}D). The result of Fig. \ref{fig:lambda_AR1}D confirms the hypothesis stated in \cite{Lacasa2010} the all $\lambda$ should be larger than $\lambda_c = \ln (3/2)$ as the correlation length is increased in the case of positively correlated increments.
\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{Chapter04_VisibilityGt/ar1_positive_lambda_vg_hvg.eps}

\caption{(color online) (A, B) Estimates of $\lambda$ for approximately exponential degree distributions of AR$(1)$ process. (C, D) $\lambda$ versus $\varphi_1$. (A, C) VG, and (B, D) HVG. When $\varphi_1>0$, $\lambda$ has a decreasing trend in the VG, while in the HVG, $\lambda$ rises when the correlation length increases. Each dot in panels C and D represents an average over 50 independent random realizations of 5000 data points. In (D), $\lambda$ values smaller than $\ln 3/2$ are highlighted by red color. \label{fig:lambda_AR1}}
\end{figure}

		In turn, when $\varphi_1 < 0$, we observe some peculiar results that seem to contradict the hypothesis stated in \cite{Lacasa2010}. According to this hypothesis, $\lambda$ should be larger than $\lambda_c$ ($\lambda > \lambda_c$) in stochastic processes in contrast to $\lambda < \lambda_c$ for chaotic maps. The results of Fig. \ref{fig:lambda_AR1}D do not support this claim when $\varphi_1$ is negative in the AR(1) model. Instead, we find a region where the slope of the exponential degree distribution is smaller than $\ln (3/2)$ (as highlighted in Fig. \ref{fig:lambda_AR1}D). This suggests that the critical value of $\ln (3/2)$ should not be understood as a general law of separating correlated stochastic from chaotic processes, which requires further investigation. 

		Working with correlated stochastic time series, further results in \cite{Manshour2015} do not adequately support the arguments of exponential degree distributions as reported in \cite{Lacasa2010}. Moe specifically, they have constructed (H)VGs for fractional time series with three different methods, a generic $1 = 1 / f^{\beta}$ noise with Fourier filtering method, a deterministic fBm process of Weierstrass-Mandelbrot function, and a stochastic fBm process with successive random addition method. The numerical analysis show that VG algorithm may not be a well-defined method to extract correlation information of a time series and its statistics is not essentially the same as that of the HVG. The degree distributions of HVGs are shown to have parabolic exponential forms with Hurst dependent fitting parameter \cite{Manshour2015}. Therefore, we conclude that hypothesis of \cite{Lacasa2010,Lacasa2014b,Ravetti2014} needs careful numerical simulations for proper interpretations. 
			
		\subsubsection{Local network properties}
		Here, we consider other local network properties besides degrees and their distributions. In the case of VGs, the local clustering coefficient $\mathcal{C}_i$ and its relationship with the degree $k_i$ have been numerically studied recently in human heartbeat data \cite{Shao2010}. Particularly, it has been observed that $\mathcal{C}(k) \sim k^{-\gamma}$ and $\gamma = 1$, pointing to a hierarchical organization of the network \cite{Albert2002}, since vertices $i$ with high $\mathcal{C}_i$ and low $k_i$ (which are most abundant) form densely connected subgraphs, indicating a strong modular structure of the VG. 

		In the case of HVG associated with an uncorrelated random series, the local clustering coefficient $\mathcal{C}_i$ can be easily deduced by means of geometrical arguments. For a given node $i$, $\mathcal{C}_i$ denotes the rate of nodes connected to $i$ that are connected between each other. In other words, we have to calculate from a given node $i$ how many nodes from those visible to $i$ have mutual visibility (triangles), normalized with the set of possible triangles $\binom{k}{2}$, where $k$ is the degree of node $i$. Based on a general rule between degree $k$ and local clustering coefficient
		\begin{equation}
		\mathcal{C}_i(k) = \frac{k-1}{\binom{k}{2}} = \frac{2}{k}, 
		\end{equation}
one obtains the local clustering coefficient distribution $p(\mathcal{C})$ as 
		\begin{equation}
		p(\mathcal{C}) = \frac{1}{3} (\frac{2}{3})^{2/\mathcal{C} -2}. 
		\end{equation}
The above theoretical result has been confirmed by uncorrelated random series \cite{Luque2009}. In addition, for HVG of a binary sequence, $p(\mathcal{C})$ has a simplified expression as reported in \cite{Ahadpour2012}. 
			
		In many cases local maxima of the underlying time series are expected to have large values of betweenness because high values often correspond to hubs in VGs which separate different parts of the series without mutual visibility contact and, thus, act as bottlenecks in the network structure, bundling a large number of shortest paths between vertices at $t < t_i$ and $t > t_i$, respectively. However, in contrast to the degree, betweenness is additionally affected by the vertex' position in the underlying time series due to a simple combinatorial effect: Considering that the majority of shortest paths that cross a vertex $i$ connect observations before and after $i$ with each other, there are more possible combinations of such points for $i$ being close to the middle of the time series than for vertices close to the edges of the record. In this respect, in a VG betweenness centrality of a vertex mixes information on the local maximality of the corresponding observation and its position within the time series.
		
		In the case of closeness centrality the position of a vertex in the time series is even more important in comparison with the value of the underlying observable. Specifically, this measure is strongly determined by the number of vertices to its left and right, respectively. In this spirit, it can be argued that in the middle of the time series, high closeness values are more likely than at its ends. As argued above, a similar (but weaker) effect contributes to betweenness and - close to the edges of the record - also to the degree (consequently, the highest degree and betweenness values can be taken by other vertices than that corresponding to the global maximum). In contrast, the local clustering coefficient is almost unaffected except for vertices very close to the beginning and end of the time series, since direct connectivity is mainly established between vertices that correspond to observations that are not very distant in time. 
		
		We note that boundary effects play an important role in the computation of the above centrality measures as that has been numerically reported in \cite{Donner2012}. In particular, it has been shown that the local clustering coefficient is almost unaffected, but the impact of boundaries on the estimated vertex properties is particularly strong for short records, which are typical in geophysical applications. Specifically, degree and other centrality properties of observations close to both ends of a time series are systematically underestimated, which may artificially alter the interpretation of the corresponding results in their geophysical context. Hence, a careful treatment and interpretation of the results of VG analysis is necessary in such cases \cite{Donner2012}.
	
				
		\subsubsection{Global network properties}
		In contrast to other approaches to complex network based time series analysis, in a VG the edge density $\rho$ is a true network characteristic rather than a parameter of the method (cf. \cite{Donner2010a,Donner2011}) for the corresponding meaning for recurrence networks). Specifically, a maximum edge density of 1 would be present if the underlying time series is globally convex (e.g., of regular parabolic shape), whereas low values indicate a strong fragmentation of the VG and, hence, irregularity of fluctuations of the underlying observable. 
		
		For a holistic characterization of a graph, $\mathcal{C}$ and $\mathcal{L}$ have attracted particular interest, since their common behavior gives rise to a mathematical evaluation of the small-world phenomenon, i.e., the emergence of real-world networks with a high degree of clustering and a short average path length \cite{Watts1998}.
		
		The topological properties of HVGs constructed from fractional Brownian motions with different Hurst indexes $H \in (0, 1)$ have been reported in \cite{Xie2011}. It is found that the clustering coefficient $\mathcal{C}$ decreases when $H$ increases. 
		
		It can be expected that the value of $\mathcal{L}$ is large when there are only few edges in the VG (low edge density) and low for a high edge density. Hence, average path length and edge density capture essentially similar properties of the underlying time series. For uncorrelated random series, the mean path length $\mathcal{L}$ of HVG has a logarithmic scaling relationship with the length of time series $N$, in particular, $\mathcal{L}(N) = 2 \ln (N) + 2 (\gamma - 1) + O(1/N)$, where $\gamma$ is the Euler-Mascheroni constant \cite{Luque2009}. In the case of fractional Brownian motions with different Hurst indexes $H \in (0, 1)$, and for fixed length of time series of $N$ points, $\mathcal{L}$ increases exponentially with $H$. In addition, $\mathcal{L}$ increases linearly with respect to $N$ when $H$ is close to 1 and in a logarithmic form when $H$ is close to 0 \cite{Xie2011}. 
	
		Besides studies on the small-world effect, the assortativity of VGs has recently attracted considerable interest. Specifically, the presence of assortative behavior implies so-called hub attraction, whereas disassortative behavior relates to hub repulsion. It has been shown that the latter is a necessary condition for the emergence of fractal structures in networks \cite{Song2006}. For example, for Brownian motion (a fractal stochastic process), hub repulsion is not present, and the resulting VGs are non-fractal, but show a scaling of the average path length with increasing net-work size as $\mathcal{L}(N)\sim \log N$, which is typical for small-world networks. In contrast, for the Conway series (a deterministic fractal) one finds hub repulsion and $\mathcal{L}\sim N^{-\beta}$, which implies the presence of a fractal VG (Lacasa et al. 2008). In this respect, the assortativity coefficient or, more specifically, the scaling of the degree correlation determines the fractality of a VG \cite{Song2006}, which is an interesting and potentially relevant property when studying fractal time series.		
	
		Of course, beyond the aforementioned characteristics there are multiple other measures one could also consider for describing the properties of VGs. This includes also measures characterizing the properties of individual edges as well as the distributions of small subgraphs (motifs). For example, the consideration of four-node subgraphs helps to show different dominant motifs ranks in the VGs of human ventricular time series, which distinguishes ventricular fibrillations from normal sinus rhythms of a subject \cite{Li2011}. Furthermore, the profiles of sequential $n$-node motifs of (H)VGs appear with characteristic frequencies which have been computed analytically for certain deterministic and stochastic dynamics \cite{Iacovacci2016}. 
		
		\subsubsection{Practical considerations}
		Many recent publications on (H)VG analysis of time series have particularly made use of data from model systems, which are characterized by rather ideal conditions for statistical analysis. Even for most practical applications presented so far, the properties of the data under study have allowed using this methodological approach without extensive precautions. However, when operating with data obtained in a geophysical context, features challenging basically any kind of time series analysis are often present, including missing data, heteroscedastic ``noise", or even uncertainties in the time domain (the latter being particularly relevant in paleo-climatology). The explicit treatment of the resulting effects on VG properties has not yet been investigated elsewhere \cite{Donner2012}. 

		As in the previous section, in the following a corresponding study is presented for the specific case of a Gaussian white noise process as a simple, but still illustrative example. It has to be emphasized that for ``real" data characterized by a non-Gaussian probability distribution function, serial dependences, or even (multi-)fractal behavior, the resulting effects could well be much stronger than in this example. A detailed study of the interdependences between such features of the data and the resulting effects of missing data and uncertainties on VG properties is, however, beyond the scope of the presented research.

		{\bf{Missing data}} 
		One important problem of many observational time series - not only in geophysics - is the presence of missing data. Since existing methods of time series analysis typically require a uniform spacing in time, this problem is most often addressed by means of interpolation or sophisticated imputation of the missing observations. In general, there is a great variety of possible approaches for such gap filling, which shall not be further discussed here. Specifically, it is not always a priori clear what method performs best under the specific conditions of the data studied.
		
		As far as VGs are concerned, the problem of missing data has not yet been explicitly addressed. Unlike many other approaches of time series analysis VGs do not explicitly require uniform sampling. Hence, missing data could be ignored when performing a corresponding analysis. However, if it is known that there must have been an observation at a given time, it could be conceptually problematic to neglect this information in the analysis. From a broader perspective, it can be argued, however, that this argument applies to all kinds of time series, since values of the considered observable (with a continuous-time variability) taken in between two subsequent observations remain always unknown, but could have a certain impact on the results of the analysis. 
		
		Looking at the issue of missing values from a complementary perspective, one can reinterpret this problem as an attack to (or just failure of) the complex network represented by the visibility graph. In complex network theory, the impact of such attacks on various types of networks has been intensively studied under the aspect of safety and robustness of infrastructures (e.g., Albert et al. 2000, Holme et al. 2002). In general, one has to distinguish random failures (corresponding to randomly miss-ing values) from intentional attacks, which typically affect the network hubs. Since for a VG, these hubs correspond to the maxima of the under-lying time series, this effect is particularly relevant for certain types of censored data, e.g., in case of measurement failures due to the limited de-tection range of a measurement device. Since is known that attacks on hubs typically have a more severe effect on the network architecture than other vertices, censoring can strongly alter the properties of the resulting VGs. However, even a random removal can have notable consequences for the VG properties on both global and local scale.
		
		In order to illustrate the effect of missing data on the properties of VGs, in the following, two different types of treatment are studied, which can be considered as opposite extreme cases. On the one hand, missing data will be simply neglected in the generation of the VG. On the other hand, since there is no information about the magnitude of the missing values, it can be a more honest solution to consider the VG as being fragmented into pieces corresponding to times before and after the missing observation, i.e., regarding the VG becoming decomposed into mutually disconnected subgraphs. It should be noted, however, that the latter approach results in the emergence of additional boundary effects. It should be emphasized that sophisticated gap filling by means of interpolation or imputation will be most likely a better strategy in many practical applications. 

		{\bf{Homo- and heteroscedastic uncertainties}} In a similar way as for the treatment of missing values, the influence of measurement uncertainties on the resulting VG properties can be studied. For convenience, homoscedastic uncertainties are modeled as an additional additive Gaussian white noise component ({\color{red} figure 8?}), whereas the heteroscedastic case is studied by considering multiplicative noise with a reasonable, simple analytical distribution ({\color{red}Figure 9?}). Again, it is found that in both cases the signal-to-noise ratio has a considerable effect by systematically shifting the distributions of vertex properties obtained for the original data towards those expected for the noise process. Note that since in the considered numerical example both signal and noise originated from mutually independent Gaussian white noise processes, there is a saturation of the KS statistics for moderate noise at values corresponding to the variance of VG properties for independent realizations of the same ``signal" process.
		
		
		{\bf{Uncertain timings}} 
		In full analogy to the case of uncertainties in the observable $x$, one can study the impact of uncertain timings $t$ on the properties of the resulting VGs. The latter is a wide-spread problem particularly in the analysis of paleoclimate time series \cite{Telford2004}. Since in the construction of VGs both observable and time enter in terms of an inequality defined by a linear relationship, it is not surprising that uncertain timing can indeed have a similar effect on the VG properties as uncertainties in the measurement itself. Figure 10 displays the corresponding results for a realization of Gaussian white noise originally observed with regular spacing, with the timings being corrupted later as
		\begin{equation}
		\tilde{t_i} = t_i + \Delta t (| 1 - 2\eta_i | - 0.5).
		\end{equation}		
		Here,  $\eta_i$ are independent realizations of a random variable with uniform distribution in $[0,1]$, and $\Delta t$ is the spacing between subsequent observations in the original data set. Note that this specific form of the time-scale corruption, which allows preserving the temporal order of observations, has been inspired by the tent map as a paradigmatic nonlinear mapping often used as an illustrative example in complex systems sciences. The obtained results demonstrate that the distribution of vertex properties of a VG are indeed affected by modifications of the time-scale, however, the changes are considerably smaller than for noisy corruptions of the measurements themselves. The reason for this is that the modification used here has been restricted by the normal sampling interval, whereas the changes induced by additive and multiplicative noise allowed for comparably larger modifications in the data.
			
		{\bf{Point process}}
		Point processes are ubiquitous in geophysics, for instance, seismic magnitude series \cite{Telesca2012}. A point process is often characterized by random time occurrence of the events and these events are clusterized because the events are neither Poissonianly nor regularly distributed over time. To check the effects of irregular timing on degree distributions, they construct two VGs from (1) the seismic series of the original random occurrence times, and the series (2) that has been substituted by regular conventional time unit. Interestingly, almost identical results have been obtained for the degree distributions, which suggests that the effects of irregular timing are not crucial. 

		The trivial connection of neighboring points in time in the (H)VG enhances the signature of structures due to autocorrelations in the record under study. Although this might be desirable for (H)VGs since some of their respective network properties are explicitly related with the presence of serial dependences (e.g., the typical scale of the degree distribution of HVGs, cf. \cite{Luque2009}), there could be situations in which one is interested in removing the corresponding effects. In such cases, it is possible to introduce a minimum time difference for two observations to be connected in the network for removing the effect of slowly decaying auto-dependences, which would correspond to the Theiler window in other concepts of nonlinear time series analysis (\cite{Theiler1986}).

		{\color{red}General question: What information can be gained from VGs? Which networks properties are (when) useful to study? }

	\subsection{Bivariate visibility graph methods}
	Despite the success, the range of applicability of (H)VGs methods has been mainly limited to univariate time series, although the most challenging problems in the area of nonlinear science concern systems that are described by multivariate time series. We can do synchronization analysis when generalizing (H)VG analysis from a univariate to bivariate time series \cite{Ahmadlou2012a,Mitra2012}. Furthermore, cross visibility algorithm helps to understand coupling and information transfer between two time series \cite{Mehraban2013}. We summarize some different approaches to characterize bivariate (H)VGs. 
	
		\subsubsection{Visibility graph similarity}
		Based on the definition of HVGs, Lacasa \textit{el al.} \cite{Lacasa2015b} proposed to transforming a multidimensional time series into an appropriately defined multiplex visibility graph. New information can be extracted from the original multivariate time series, with the aims of describing signals in graph-theoretical terms or to construct novel feature vectors to feed automatic classifiers in a simple, accurate and computationally efficient way. 
		
		Consider a $M$-dimensional real valued time series $\{{\bf{x}}(t) \}_{t=1}^{n}$, with ${\bf{x}}(t) = (x^{[1]}(t), x^{[2]}(t), \dots, x^{[M]}(t)) \in \mathbb{R}^M$ for any value of $t$, measured empirically or extracted from a $M$-dimensional, either stochastic or deterministic dynamical system. An $M$-layer multiplex visibility graph $\mathcal{M}$ is then constructed, where layer $\alpha$ corresponds to the HVG associated to the time series of state variable $\{ x^{[\alpha]}(t) \}_{t=1}^{N}$. Note that $\mathcal{M}$ is represented by the vector of adjacency matrices of its layers $\mathcal{A} = \{A^{[1]}, A^{[2]}, \dots, A^{[M]}\}$, where $A^{[\alpha]} = \{a_{ij}^{[\alpha]} \}$ is the adjacency matrix of layer $\alpha$. Such a mapping builds a bridge between multivariate series analysis and the recent developments in the theory of multilayer networks \cite{Boccaletti2014}, making it possible to employ the structural descriptors introduced to study multiplex networks as a toolbox for the characterisation of multivariate signals. 
		
		Two measures have been proposed to capture, respectively, the abundance of single edges across layers and the presence of inter-layer correlations of node degrees \cite{Lacasa2015b}, which help to characterize information shared across variables (layers) of the underlying high dimensional system. Simply speaking, we compute these two measures by Eqs. (\ref{eq:RNmultiplex}, \ref{eq:RNmultiplexW}) based on the adjacency matrices of HVGs. More specifically, the first measure is the average edge overlap (Eq. \eqref{eq:RNmultiplexW}),  	
%		The first measure is the average edge overlap
%\begin{equation} \label{eq:VGmultiplexW}
%\omega = \frac{\sum_i\sum_{j>i} \sum_{\alpha}a_{ij}^{[\alpha]}}{M \sum_i\sum_{j>i}(1-\delta_{0, \sum_{\alpha}a_{ij}^{[\alpha]}})}
%\end{equation}
which computes the expected number of layers of the multiplex on which an edge is present. Note that $\omega$ takes values in $[1/M, 1]$ and in particular $\omega = 1/M$ if each edge $(i, j)$ exists in exactly one layer, i.e. if there exist a layer $\alpha$ such that $a_{ij}^{[\alpha]} = 1$ and $a_{ij}^{[\beta]} = 0 \forall \beta \neq \alpha$, while $\omega = 1$ only if all the $M$ layers are identical. As a consequence, the average edge overlap of a multiplex VG can be used as a proxy of the overall coherence of the original multivariate time series, with higher values of $\omega$ indicating high correlation in the microscopic structure of the signal. 

		The second measure proposed in \cite{Lacasa2015b} is to quantify the presence of interlayer correlation between the degrees of the same node at two different layers. More specifically, given a pair of layers $\alpha$ and $\beta$ of $\mathcal{M}$, respectively characterized by the degree distributions $P(k^{[\alpha]})$ and $P(k^{[\beta]})$, the interlayer correlation is defined by the mutual information $I_{\alpha, \beta}$ (Eq. \eqref{eq:RNmultiplex}) % as 
%		\begin{equation} \label{eq:VGmultiplex}
%		I_{\alpha, \beta} = \sum_{k^{[\alpha]}} \sum_{k^{[\beta]}} P(k^{[\alpha]}, k^{[\beta]}) \log \frac{P(k^{[\alpha]}, k^{[\beta]})}{P(k^{[\alpha]}) P(k^{[\beta]}) }, 
%		\end{equation}
		where $P(k^{[\alpha]}, k^{[\beta]}) $ is the joint probability to find a node having degree $k^{[\alpha]}$ at layer $\alpha$ and degree $k^{[\beta]}$ at layer $\beta$. The higher $I_{\alpha, \beta}$ the more correlated the degree distributions of the two layers and therefore the structure of the associated time series. Then the average of $I_{\alpha, \beta}$ over every pair of layers of $\mathcal{M}$ gives a scalar variable $I = \left < I_{\alpha, \beta}\right>_{\alpha, \beta}$, which captures the amount of information flow in the multivariate time series. 
		
		Note that the second measure above gives a weighted correlation matrix of $M \times M$ and each entry is represented by $I_{\alpha, \beta}$. That means the original $M$-dimensional time series is transformed to a weighted graph of $M$ nodes, where each node represents one layer and the weights of the edge denote the magnitude of mutual information computed from the associated (H)VG degree distributions.  
						
		\subsubsection{Joint and excess degrees} \label{subsec:jointdegreeVG}
		In addition, we introduce some network-theoretic quantities to quantify the asymmetries in bivariate time series \cite{Zou2014}. Following the notations as described for multiplex (H)VG, we restrict our analysis with $2$-dimensional time series which resulting two layers $\alpha$ and $\beta$. For $\{x^{[\alpha]}(t)\}_{t=1}^{N}$ and $\{x^{[\beta]}(t)\}_{t=1}^{N}$, we again denote two (H)VGs with adjacency matrices $A^{[\alpha]}$ and $A^{[\beta]}$, respectively. Note that the sets of vertices are the same for both subgraphs, with differences exclusively in the set of edges.

Based on the thus obtained (H)VGs, we proceed as follows:
\begin{enumerate}
\item From the two (H)VGs, we have two sets of neighbors, $\mathcal{N}_{x^{[\alpha]}, x^{[\beta]}}(t) = \left\{ A_{t, t_j}^{{[\alpha]}, {[\beta]}} \equiv 1, t_j \in \left\{1,\ldots, N \right\}/\left\{t\right\} \right\}$ for each time $t \in \left\{1,\ldots, N \right\}$. The \textit{degree sequences} are then defined as
\begin{align}
&k_{{[\alpha]}, {[\beta]}}(t) = \# \mathcal{N}_{{[\alpha]}, {[\beta]}}(t) = \sum_{t_j} A_{t,t_j}^{{[\alpha]}, {[\beta]}}(t).
\end{align}

\item The \textit{joint degree sequence}
\begin{align} \label{eq:jointK}
&k^{joint}(t) = \# \left( \mathcal{N}_{{[\alpha]}}(t) \cap \mathcal{N}_{{[\beta]}}(t) \right) = \sum_{t_j} A_{t,t_j}^{{[\alpha]}}(t)\cdot A_{t,t_j}^{{[\beta]}}(t).
\end{align}
gives the number of common neighbors of a vertex corresponding to time $t$ in both sequences.

Notably, we can define $k^{joint}(t)$ as the degree sequence of a \textit{joint (horizontal) visibility graph} combining the visibility criteria for two distinct time series. Here, the adjacency matrix is defined by the point-wise multiplication of the individual (H)VGs' adjacency matrices. This idea is conceptually related to the concept of joint recurrence plots encoding the simultaneous recurrence of two dynamical systems in their respective phase spaces \cite{romano2004}. As in the latter case, generalizing joint (H)VGs to the case of more than two time series is straightforward, but will not be considered here given the bivariate nature of the data under study.

\item In a similar spirit as the joint degree sequence, we can quantify the number of edges associated with time $t$, which connect to vertices contained in $\mathcal{N}_{x^{[\alpha]}}(t)$ but \textit{not} in $\mathcal{N}_{x^{[\beta]}}(t)$, or vice versa. More specifically,
\begin{align}
k_{{[\alpha]}}^{O}(t) & = \# \left( \mathcal{N}_{{[\alpha]}}(t) \cap \overline{\mathcal{N}_{{[\beta]}}(t)} \right)  =  \sum_{t_j} A_{t,t_j}^{{[\alpha]}}(t)\cdot \left( 1-A_{t,t_j}^{{[\beta]}}(t) \right) \\
k_{{[\beta]}}^{O}(t) & = \# \left( \mathcal{N}_{{[\beta]}}(t) \cap \overline{\mathcal{N}_{{[\alpha]}}(t)} \right) = \sum_{t_j} A_{t,t_j}^{{[\beta]}}(t)\cdot \left(1-A_{t,t_j}^{{[\alpha]}}(t) \right)
\end{align}
(where $\overline{\mathcal{N}_{{[\alpha]}, {[\beta]}}(t)}=\{1,\ldots,T\}\cap \left\{ \mathcal{N}_{{[\alpha]}, {[\beta]}}(t)\cup t\right\}$ is the complementary set of $\mathcal{N}_{{[\alpha]}, {[\beta]}}(t)$) measures the number of neighbors that belong \textit{only} to $\mathcal{N}_{{[\alpha]}}(t)$ or $\mathcal{N}_{{[\beta]}}(t)$, respectively. In what follows, $k_{{[\alpha]}, {[\beta]}}^{O}(t)$ will be referred to as the \textit{conditional degree sequences}. By definition,
\begin{align}\label{eq:nad}
k_{{[\alpha]}, {[\beta]}}^{O}(t)=k_{{[\alpha]}, {[\beta]}}(t)-k^\textrm{joint}(t).
\end{align}
\end{enumerate}

Based on the latter definitions, we can proceed in a similar way as in Eq.~\eqref{eq:nad} and compute the following properties:
\begin{align} \label{eq:deltaK}
\Delta k(t) &= k_{{[\alpha]}}^{O}(t) - k_{{[\beta]}}^{O}(t) = k_{{[\alpha]}}(t) - k_{{[\beta]}}(t) \\ \label{eq:deltaReK}
\Delta_{rel} k(t) &= \Delta k(t)/\left(k_{{[\alpha]}}(t) + k_{{[\beta]}}(t) \right).
\end{align}
The \textit{excess degree} $\Delta k(t)$ quantifies how much ``more convex'' the fluctuations of $A^{[\alpha]}$ are in comparison with $A^{[\beta]}$ around a given time $t$ (i.e., how many more or less visibility connections the observation of $A^{[\alpha]}$ at time $t$ obeys in comparison with $A^{[\beta]}$). By additionally considering the \textit{relative excess degree} $\Delta_{rel} k(t)$ normalized by the sum of the individual degrees, we obtain a measure that does not exhibit marked sensitivity with respect to the actual degrees $k_{{[\alpha]}, {[\beta]}}$, which may considerably vary over time according to the statistical and dynamical characteristics of the data.

In  Sec. \ref{subsec:sunspotsAsym}, we will summarize the applications of $\Delta k(t)$ and $\Delta_{rel} k(t)$ to characterize the North--South asymmetry of solar activity \cite{Zou2014}, which provide many nonlinear properties that have not been obtained by other methods. Notably, our approach is conceptually related with recently developed (H)VG-based tests for time series irreversibility, which compare (among others) degree distributions obtained when considering edges to past and future observations separately \citep{Donges2013}.


	\subsection{Decomposition of visibility graphs} \label{sec:timeIRvg}
		\subsubsection{Time-directed visibility graphs and characterizations}
		So far in the literature the family of visibility graphs are undirected, as visibility did not have a predefined temporal arrow. These can be made directed by again assigning to the links a time arrow, which result in the so called directed VGs (DVGs) and directed HVGs (DHVGs). Accordingly, a link between $i$ and $j$ (where time ordering yields $i< j$) generates an outgoing link for $i$ and an ingoing link for $j$. Therefore, the degree $k(t)$ of the node $t$ is now split into an ingoing degree $k_{in}(t)$, and an out-going degree, such that $k(t)= k_{in}(t)+k_{out}(t)$. The ingoing degree $k(t)$ is defined as the number of links of node $t$ with other past nodes associated with data in the series (that is, nodes with $ t' < t$). Conversely, the outgoing degree $k_{out}(t)$, is defined as the number of links with future nodes. {\color{red}A graph illustration of the method?} Then we define the \textit{in} and \textit{out} (or ingoing and outgoing) degree distributions of a DVG (DHVG) as $P_{out}(k) \equiv P(k_{out} = k)$ and $P_{in}(k) \equiv P(k_{in} = k)$, respectively. An important property at this point is that the ingoing and outgoing degree sequences are interchangeable under time series reversal. 
		
		Given the adjacency matrix $A_{ij}$ of a (H)VG, the degree {$k_i =\sum_{j} A_{ij}$} measures the number of edges incident to a given vertex {$i$}. Then, Donges \textit{et~al.} \cite{Donges2013} used different notations for in- and out-degree sequences. More specifically, they decompose this quantity $k_i$ for a vertex corresponding to a measurement at time $t_i$ into contributions due to other vertices in the past and future of $t_i$,
\begin{eqnarray} \label{eq:kvin}
k_i^r &= \sum_{j<i} A_{ij},\\ \label{eq:kvout}
k_i^a &= \sum_{j>i} A_{ij}
\end{eqnarray}
with $k_i=k_i^r+k_i^a$, being referred to as the \emph{retarded} and \emph{advanced degrees}, respectively, in the following. Note that $k_i^r$ and $k_i^a$ correspond to the respective in- and out-degrees of time-directed (H)VGs as recently defined in~\cite{Lacasa2012}. While the degrees of an individual vertex can be significantly biased due to the finite data~\cite{Donner2012}, the resulting frequency distributions of retarded and advanced degrees are equally affected. 

		The local clustering coefficient $\mathcal{C}_i = {k_i \choose 2}^{-1} \sum_{j,k} A_{ij} A_{jk} A_{ki}$ is another vertex property of higher order characterising the neighbourhood structure of vertex $i$~\cite{Newman2003}. Here, for studying the connectivity due to past and future observations separately, we define the \emph{retarded} and \emph{{advanced local clustering coefficients}}
\begin{eqnarray} \label{eq:cvin}
\mathcal{C}_i^r &= {k_i^r \choose 2}^{-1} \sum_{{j<i,k<i}} A_{ij} A_{jk} A_{ki},\\ \label{eq:cvout}
\mathcal{C}_i^a &= {k_i^a \choose 2}^{-1} \sum_{{j>i,k>i}} A_{ij} A_{jk} A_{ki}.
\end{eqnarray}
Hence, both quantities measure the probability that two neighbours in the past (future) of observation $i$ are mutually visible themselves. Note that the decomposition of $\mathcal{C}_i$ into retarded and advanced contributions is not as simple as for the degree and involves degree-related weight factors and an additional term combining contributions from the past and future of a given vertex.

		Finally, {we note that} other measures characterising complex networks on the local (vertex/edge) as well as global scale could be used for similar purposes as {those} studied in this work. However, {since} path-based network characteristics {(e.g.,} closeness, betweenness, or average path length{)} cannot be easily decomposed into retarded and advanced contributions, the approach followed here is mainly restricted to neighbourhood-based network measures like degree, local and global clustering coefficient, or network transitivity. As a possible solution, instead of decomposing the network properties, the whole edge set of a (H)VG could be divided into two disjoint subsets that correspond to visibility connections forwards and backwards in time, as originally proposed by Lacasa~\textit{et~al.}~\cite{Lacasa2012}. For these directed (forward and backward) (H)VGs, also the path-based measures can be computed separately and might provide valuable information. 
		
		\subsubsection{Tests for time series irreversibility}
		Testing for nonlinearity of time series has been of great interest. Various approaches have been developed for identifying signatures of different types of nonlinearity as a necessary precondition for the possible emergence of chaos. Since linearity of Gaussian processes directly implies time-reversibility \cite{Weiss1975,Lawrance1991,Diks1995}, nonlinearity results (among other features) in an asymmetry of certain statistical properties under time-reversal~\cite{Theiler1992}. Therefore, studying reversibility properties of time series is an important alternative to the direct quantitative assessment of nonlinearity~\cite{Voss1998}. In contrast to classical higher-order statistics requiring surrogate data techniques~\cite{Theiler1992}, most recently developed approaches for testing irreversibility have been based on symbolic dynamics~\cite{Daw2000,Kennel2004,Cammarota2007} or statistical mechanics concepts~\cite{Costa2005,Porporato2007,Roldan2010}. 
		
		The time series reversibility has the following definition: a time series $\mathcal{S} = \{ x_1, x_2, \dots, x_n \}$ is called statistically time reversible if the time series $\mathcal{S}^{\ast} = \{x_{-1}, x_{-2}, \dots, x_{-n} \}$ has the same joint distribution as $\mathcal{S}$. Therefore, time reversibility implies stationarity \cite{Lawrance1991}. By this definition, time series reversibility reduces to the equivalence between forward and backward statistics and hence, nonstationary series are infinitely irreversible and therefore $\mathcal{S}$ and $\mathcal{S}^{\ast}$ have different statistics that increase over time \cite{Weiss1975}. 

		In many applications, one can actually quantify different kinds of time asymmetries in the underlying dynamics on nonstationary processes. Following the previous work \cite{Lacasa2012}, the topological properties of (H)VGs associated to several types of nonstationary processes have been proposed to quantify the different degrees of irreversibility of several nonstationary processes \cite{Lacasa2015}. Furthermore, they take advantage of the fact that the topological properties of these graphs are effectively invariant under time shift for large classes of nonstationary processes, which allows to introduce the concept of visibility graph stationarity. This in turn allows to compare to extract meaningful information on the time asymmetry of nonstationary processes. 

		More general, Lacasa \textit{et al.} \cite{Lacasa2015} defined time series reversibility in terms of (H)VGs in the following: a time series $\mathcal{S} = \{ x(t) \}_{t=1}^n$ is said to be (order $p$) (H)VG reversible if and only if, for large $n$, the order $p$ block ingoing and outgoing degree distribution estimates of the (H)VG associated to $\mathcal{S}$ are asymptotically identical, i.e., 
\begin{equation}
P_{in}(k_1 k_2 \dots k_p) = P_{out}(k_1 k_2 \dots k_p).
\end{equation}
This property yields that the ingoing and outgoing degree sequences of the original and time reversed series have the same distribution, namely, 
\begin{equation}
P_{in}(k) [\mathcal{S}] = P_{out}(k) [ \mathcal{S}^{\ast}]; P_{out}(k) [\mathcal{S}] = P_{in}(k)[\mathcal{S}^{\ast}],  
\end{equation}
where $\mathcal{S}^{\ast} = \{ x_{n+1-t} \}_{t=1}^{n}$ represents the time reversed series. For time series, we assess how close the system is to reversibility by quantifying the distance between $P_{in}$ and $P_{out}$.  

		The distance between the \textit{in} and \textit{out} degree distributions has been calculated by the Kullback-Leibler divergence (KLD) \cite{Lacasa2012,Lacasa2015}, which is introduced in information theory as a measure to distinguish between two probability distributions. More specifically, the KLD between these two distributions is
\begin{equation}\label{eq:KLD}
D[P_{out}(k) \| P_{in}(k)] = \sum_{k} P_{out}(k) \log \frac{P_{out}(k)}{P_{in}(k)}. 
\end{equation}
This measure vanishes if and only if the outgoing and ingoing degree probability distributions of a time series are identical. Then the (H)VG reversibility is redefined if the following expression holds:
\begin{equation}
\lim_{n \to \infty} D[P_{out}(k) \| P_{in}(k)] = 0. 
\end{equation}		
Truly irreversible process have positive values of Eq. \eqref{eq:KLD} in the limit of large $n$ \cite{Lacasa2012}.

		Lacasa \textit{et al.}\cite{Lacasa2012, Lacasa2015} conjecture that the information stored in the \textit{in} and \textit{out} distributions takes into account the amount of time irreversibility of the associated series. More precisely, they claim that this can be measured, in a first approximation, as the distance (in a distributional sense) between the \textit{in} and \textit{out} degree distributions ($P_{in}(k)$ and $P_{out}(k)$). If needed, higher order measures can be used, such as the corresponding distance between the \textit{in} and \textit{out} degree-degree distributions ($P_{in}(k, k')$ and $P_{out}(k, k')$). These are defined as the \textit{in} and \textit{out} joint degree distributions of a node and its first neighbors, describing the probability of an arbitrary node whose neighbor  has degree $k'$ to have degree $k$. Namely, we compare the outgoing degree distribution in the actual (forward) series $P_{k_{out}} = P_{out}(k)$ with the corresponding probability in the time-reversed (or backward) time series, which is equal to the probability distribution of the ingoing degree in the actual process $P_{k_{out}} = P_{in}(k)$. 
		
		Therefore, by calculating Eq. \eqref{eq:KLD}, Lacasa \textit{et al.}\cite{Lacasa2012} have shown that one can correctly distinguish between reversible and irreversible stationary time series, including analytical and numerical studies of its performance for: (i) reversible stochastic processes (uncorrelated and Gaussian linearly correlated), (ii) irreversible stochastic processes, (iii) reversible (conservative) and irreversible (dissipative) chaotic maps, and (iv) dissipative chaotic maps in the presence of noise. 	
		
		Notice that the majority of previous methods to estimate time series irreversibility generally proceed by first making a (somewhat ad hoc) local symbolization of the series, coarse-graining each of the series data into a symbol (typically, an integer) from an ordered set ~\cite{Daw2000,Kennel2004,Cammarota2007,Costa2005,Porporato2007,Roldan2010}. The method based on directed (H)VGs here lacks an ad hoc symbolization process, which may in principle take into account multiple scales. The unnecessary requirement of symbolization is desirable if we want to tackle complex signal and hence it can be applied directly to any kind of real-valued time series.  

		We note that (H)VG reversibility varies depending on the detailed properties of particular processes \cite{Lacasa2015}, which calls for careful interpretations. For instance, both analytical calculations and numerical simulations show that unbiased additive random walks, while nonstationary, are both (H)VG  stationary and (H)VG time reversible. On the other hand, biased memoryless additive random walks are HVG irreversible with finite irreversibility measures that quantify the degree of time asymmetry, while these are still VG reversible, as VGs are invariant under superposition of linear trends in the original data. Numerics suggest that HVGs can capture, for both finite and infinite series size, the irreversible nature of non-Markovian additive random walks, where as VGs are only able to do so for finite series. For multiplicative random walks, the processes are HVG reversible if the process is akin to an unbiased additive process in logarithmic space, and time irreversible if the process reduces to a biased additive process in logarithmic space. Finally, the VGs capture the time irreversible character of multiplicative random walks, yielding finite values in the unbiased case and asymptotically diverging quantities in the biased case. Furthermore, these conclusions are based on the limit of infinitely long time series $n \to \infty$ and finite size time series always yields finite, non-null values of HVG and VG irreversibility \cite{Xiong2018}, which needs a proper test justifying the statistical significance. 
		
		While the results of KLD measure for reversible and irreversible dynamics quantitatively differ in several orders of magnitude, a statistical test is required. Lacasa \textit{et al.} proposed to address the statistical significance by surrogate techniques as follows: one first proceeds to shuffle the series under study in order to generate a randomized resampled data set with the same underlying probability density. This resampled series, whose irreversibility measure is asymptotically null, is considered as the null hypothesis of the test. Taking a slightly different algorithm, Donges \textit{et~al.} \cite{Donges2013} have thoroughly extended this idea and provided a set of rigorous statistical tests for time series irreversibiliby, which can be formulated based on both standard and horizontal VGs and utilise different network properties. Specifically, for both VGs and HVGs, network degrees as well as local clustering coefficients can be decomposed into contributions from past and future observations (Eqs. (\ref{eq:kvin}-\ref{eq:cvout})), which allows studying some of the time series' statistical properties under time-reversal. They find statistically significant deviations between the distributions of time-ordered vertex properties for nonlinear systems for which the absence of time-reversal symmetry is known, but not for linear systems. 
		
		
		Time-{ir}reversibility of a stationary stochastic process or time series $\{x_i\}$ requires that for arbitrary $n$ and $m$, the tuples $(x_n,x_{n+1},\dots,x_{n+m})$ and $(x_{n+m},x_{n+m-1},\dots,x_n)$ have the same joint probability distribution~\cite{Lawrance1991}. Instead of testing this condition explicitly (which is practically unfeasible in most situations due to the necessity of estimating high-dimensional probability distribution functions from a limited amount of data), for detecting time series irreversibility it can be sufficient to compare the distributions of certain statistical characteristics obtained from both vectors (e.g.,~\cite{Tong1990}). Following the decomposition of vertex properties into time-directed contributions proposed above, (H)VG-based methods appear particularly suited for this purpose. Specifically, in the following we will utilise the frequency distributions {$p(k^{r})$ and $p(k^{a})$ ($p(\mathcal{C}^{r})$ and $p(\mathcal{C}^{a})$)} of retarded and advanced {vertex properties} as representatives for the statistical properties of the time series when viewed forward and backward in time. 

		In the case of time-reversibility, we conjecture that both sequences {$\{k_i^{r}\}$ and $\{k_i^{a}\}$} (or {$\{\mathcal{C}_i^{r}\}$ and $\{\mathcal{C}_i^{a}\}$}) should be drawn from the same probability distribution, because the visibility structure towards the past and future of each observation has to be statistically equivalent. In turn, for an irreversible (i.e., nonlinear) process, we expect to find statistically significant deviations between the probability distributions of retarded and advanced characteristics. 

		As an alternative to the Kullback-Leibler distance between the empirically observed distribution functions used by Lacasa~\textit{et~al.}~\cite{Lacasa2012}, we propose utilising some standard statistics for testing the homogeneity of the distribution of random variables between two independent samples. In this framework, rejecting the null hypothesis that {$\{k_i^{r}\}$ and $\{k_i^{a}\}$} ({$\{\mathcal{C}_i^{r}\}$ and $\{\mathcal{C}_i^{a}\}$}) are drawn from the same probability distribution, respectively, is equivalent to rejecting the null hypothesis that the time series under investigation is reversible. {Since for sufficiently long time series (representing the typical dynamics of the system under study), the available samples of individual vertex properties approximate the underlying distributions sufficiently well, we can (despite existing correlations between subsequent values)} consider the Kolmogorov-Smirnov (KS) test {for} testing this null hypothesis. {Specifically}, a small $p$-value of the KS test statistic (e.g., $p<0.05$) implies that the time series has likely been generated by an irreversible stochastic process or dynamical system. Even more, the{se} $p$-values are distribution-free in the limit of $N\to\infty$. {Neglecting possible effects of the intrinsic correlations between the properties of subsequent vertices on the estimated $p$-values (which shall be addressed in future research), this} implies that we do \textit{not} need to construct surrogate time series for obtaining critical values of our test statistics as in other {ir}reversibility tests. Note that other (not network-related) statistical properties sensitive to the time-ordering of observations could also be exploited for constructing similar statistical tests for time series irreversibility \cite{Donges2013}.  {\color{red} a figure emphasize the statistical significance aspect? both $k$ and $\mathcal{C}$ series for this purpose.} Recently, a combination of Kullback-Leibler distance between the ingoing and outgoing degree sequences and the so-called inversion number of the permutation of the original time series has been proposed to characterize the asynchronous patterns of time irreversibility \cite{Yang2018}. 

		Utilising standard as well as horizontal VGs for discriminating between the properties of observed data forwards and backwards in time has at least two important benefits: (i) {Unlike for some classical tests (e.g.,~\cite{Theiler1992}),} the reversibility properties are examined without the necessity of constructing surrogate data. Hence, the proposed approach saves considerable computational costs in comparison with {such methods} and, more importantly, avoids the problem of selecting a particular type of surrogates. Specifically, utilising the KS test statistic or a comparable two-sample test for the homogeneity (equality) of the underlying probability distribution functions directly supplies a $p$-value for the associated null hypothesis that the considered properties of the data forward and backward in time are statistically indistinguishable. (ii) The proposed approach {is applicable} to data with non-uniform sampling {(common in areas like} palaeoclimate~\cite{Donner2012} or astrophysics) {and} marked point processes (e.g., earthquake catalogues~\cite{Telesca2012}). For {such} data, constructing surrogates for {non}linearity tests in the most common way {using} Fourier-based techniques is a challenging task, {which is avoided by} {(H)}VG-based methods. 

		We emphasize that our method exploits the time-information explicitly used in constructing (H)VGs. Other existing time series network methods (e.g., recurrence networks \cite{Marwan2009,Donner2010a,Donner2011}) not exhibiting this feature cannot be used for the same purpose. Furthermore, there are methodological questions such as the impacts of sampling, observational noise, and intrinsic correlations in vertex characteristics as well as a systematic comparison to existing methods for testing time series {ir}reversibility that need to be systematically addressed in future research. Furthermore, {(H)}VG-based methods are generally faced with problems such as boundary effects and the ambiguous treatment of missing data \cite{Donner2012}, which call for further investigations. 
		
		However, path-based measures of (H)VGs are known to be strongly influenced by boundary effects \cite{Donner2012}, so that they could possibly lose their discriminative power for {ir}reversibility tests. In addition, irreversibility tests have been conducted for various real valued time series. Examples include neuro-physiological EEG recordings \cite{Donges2013}, mean temperature anomaly series \cite{Xie2014}, financial time series \cite{Flanagan2016}, oil-water two phase flows \cite{Meng2016a}, meteorological stream flow fluctuation \cite{Serinaldi2016}, correlated fractal processes \cite{Xiong2018}, seismic sequences of Mexican subduction zone \cite{Telesca2018}. 
		
%	\subsection{{\color{red} Identifying transitions by visibility graph method? Any references?}}
